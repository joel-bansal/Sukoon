{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRiieJ58Vfl4",
        "outputId": "6774c0c5-457f-42f8-d811-2837164c5d85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAnTJSMR_mzi",
        "outputId": "a8ab9930-1286-4412-cae1-77c648dde8fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Cannot install langchain-core, langchain-core==0.1.42, langchain-openai==0.1.3, langchain==0.1.16, langgraph==0.0.39, langgraph==0.0.40, langgraph==0.0.41, langgraph==0.0.42, langgraph==0.0.43, langgraph==0.0.44, langgraph==0.0.45, langgraph==0.0.46, langgraph==0.0.47, langgraph==0.0.48, langgraph==0.0.49, langgraph==0.0.50, langgraph==0.0.51, langgraph==0.0.52, langgraph==0.0.53, langgraph==0.0.54, langgraph==0.0.55, langgraph==0.0.56, langgraph==0.0.57, langgraph==0.0.58, langgraph==0.0.59, langgraph==0.0.60, langgraph==0.0.61, langgraph==0.0.62, langgraph==0.0.63, langgraph==0.0.64, langgraph==0.0.65, langgraph==0.0.66, langgraph==0.0.67, langgraph==0.0.68, langgraph==0.0.69, langgraph==0.1.1, langgraph==0.1.10, langgraph==0.1.11, langgraph==0.1.12, langgraph==0.1.13, langgraph==0.1.14, langgraph==0.1.15, langgraph==0.1.16, langgraph==0.1.17, langgraph==0.1.19, langgraph==0.1.2, langgraph==0.1.3, langgraph==0.1.4, langgraph==0.1.5, langgraph==0.1.6, langgraph==0.1.7, langgraph==0.1.8, langgraph==0.1.9, langgraph==0.2.0, langgraph==0.2.1, langgraph==0.2.10, langgraph==0.2.11, langgraph==0.2.12, langgraph==0.2.13, langgraph==0.2.14, langgraph==0.2.15, langgraph==0.2.16, langgraph==0.2.17, langgraph==0.2.18, langgraph==0.2.19, langgraph==0.2.2, langgraph==0.2.20, langgraph==0.2.21, langgraph==0.2.22, langgraph==0.2.23, langgraph==0.2.24, langgraph==0.2.25, langgraph==0.2.26, langgraph==0.2.27, langgraph==0.2.28, langgraph==0.2.3, langgraph==0.2.32, langgraph==0.2.33, langgraph==0.2.34, langgraph==0.2.35, langgraph==0.2.36, langgraph==0.2.37, langgraph==0.2.38, langgraph==0.2.39, langgraph==0.2.4, langgraph==0.2.40, langgraph==0.2.41, langgraph==0.2.42, langgraph==0.2.43, langgraph==0.2.44, langgraph==0.2.45, langgraph==0.2.5, langgraph==0.2.6, langgraph==0.2.7, langgraph==0.2.8 and langgraph==0.2.9 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install -r requirements.txt\n",
        "# langgraph\n",
        "# langchain-community\n",
        "# langchain-core\n",
        "# langchain-openai\n",
        "# langchain\n",
        "# langgraph-sdk\n",
        "# langgraph-checkpoint-sqlite\n",
        "# langsmith\n",
        "# langchainhub==0.1.15\n",
        "# langchain-openai==0.1.3\n",
        "# langchain==0.1.16\n",
        "# langchain-core==0.1.42\n",
        "# pygraphviz==1.12\n",
        "# llama-index\n",
        "# fastapi\n",
        "# uvicorn\n",
        "\n",
        "!pip install -qU langgraph langchain-community langchain-core langchain-openai langchain langgraph-sdk langgraph-checkpoint-sqlite langsmith langchainhub==0.1.15 langchain-openai==0.1.3 langchain==0.1.16 langchain-core==0.1.42 pygraphviz==1.12 llama-index fastapi uvicorn openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU python-dotenv # Install the python-dotenv package\n",
        "!pip install -qU llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SHjk8vrT8nf",
        "outputId": "8b9e56b3-8246-45d4-b439-4d5ae180a656"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph # Install langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKCnIS90VWa6",
        "outputId": "c6ead3b8-7149-4a4b-e1ae-2312cd8fe4a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Using cached langgraph-0.2.45-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 (from langgraph)\n",
            "  Using cached langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.0 (from langgraph)\n",
            "  Using cached langgraph_checkpoint-2.0.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.32 (from langgraph)\n",
            "  Using cached langgraph_sdk-0.1.35-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.27.2)\n",
            "Collecting httpx-sse>=0.4.0 (from langgraph-sdk<0.2.0,>=0.1.32->langgraph)\n",
            "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10.10)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.2.2)\n",
            "Downloading langgraph-0.2.45-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.35-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: httpx-sse, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "Successfully installed httpx-sse-0.4.0 langchain-core-0.3.15 langgraph-0.2.45 langgraph-checkpoint-2.0.2 langgraph-sdk-0.1.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVwqsBIO_mzo"
      },
      "source": [
        "# SETUP Enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y3me_mis_mzq"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "# Viewing Queries and Events Using Logging\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "# loading env variables\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai # Install the langchain_openai package\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0LEu6nEULiJ",
        "outputId": "b0287086-82b6-4806-de4a-19475c8c0980"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Using cached langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.15)\n",
            "Collecting openai<2.0.0,>=1.54.0 (from langchain_openai)\n",
            "  Using cached openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain_openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_openai) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "Successfully installed langchain_openai-0.2.6 openai-1.54.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lgsu5uM2_mzs",
        "outputId": "ada8419c-9551-411a-a551-37655375706f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I'm a GPT model, and you're likely accessing me through an API key provided by a platform or service that uses OpenAI's technology. This key allows you to interact with the model and receive responses to your queries. If you have any questions or need assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0,\n",
        "    max_tokens=None,\n",
        ")\n",
        "print(llm.invoke(\"you are gpt model only right which im using via a key?\").content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC4yJ5O__mzt"
      },
      "source": [
        "# Version 1 No CBT/DBT Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1Notf05A_mzt",
        "outputId": "6a4547c5-746c-41e6-e026-3880eba251a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1543ab265c7f>\u001b[0m in \u001b[0;36m<cell line: 161>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# i = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bot: Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph.message import AnyMessage, add_messages\n",
        "from typing import Literal, Annotated\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from typing import TypedDict, List\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "import yaml\n",
        "import csv\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "# Define the state\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "BOT_USED = \"\"\n",
        "# Load prompts from YAML\n",
        "def load_prompts(file_path='/content/drive/MyDrive/prompts.yaml'):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return yaml.safe_load(file)\n",
        "\n",
        "prompts = load_prompts()\n",
        "\n",
        "# Initialize OpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "# Define prompts for different agents\n",
        "planner_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a planner agent that decides which specialized agent to call based on the user's input. Respond with one of 'suicide_prevention', 'conversational', 'anger_management', 'motivational', or 'mindfulness' based on the user's emotion.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "conversational_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['empathetic_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "suicide_prevention_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['suicide_prevention_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "\n",
        "anger_management_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['anger_prevention_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "motivational_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['motivational_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "mindfulness_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['mindfulness_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# Define router\n",
        "def route_query(state: State):\n",
        "    print(state)\n",
        "    class RouteQuery(BaseModel):\n",
        "        \"\"\"Route a user query to the most relevant node.\"\"\"\n",
        "        route: Literal[\"conversational\", \"suicide_prevention\", \"anger_management\", \"motivational\", \"mindfulness\"] = Field(\n",
        "            ...,\n",
        "            description=\"Choose the appropriate agent based on the user's emotions.\"\n",
        "        )\n",
        "\n",
        "    structured_llm_router = model.with_structured_output(RouteQuery)\n",
        "    question_router = planner_prompt | structured_llm_router\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    resp = question_router.invoke({\"input\": last_message})\n",
        "    return resp.route\n",
        "\n",
        "# Define agent functions\n",
        "def run_conversational_agent(state: State):\n",
        "    global BOT_USED  # Declare BOT_USED as global\n",
        "    print(\"Running conversational agent\")\n",
        "    convo_model = conversational_prompt | model\n",
        "    response = convo_model.invoke(state[\"messages\"])\n",
        "    BOT_USED = \"conversational_agent\"  # Assign to global variable\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def run_suicide_prevention_agent(state: State):\n",
        "    global BOT_USED  # Declare BOT_USED as global\n",
        "    print(\"Running suicide prevention agent\")\n",
        "    concern_model = suicide_prevention_prompt | model\n",
        "    response = concern_model.invoke(state[\"messages\"])\n",
        "    BOT_USED = \"suicide_agent\"  # Assign to global variable\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def run_anger_management_agent(state: State):\n",
        "    global BOT_USED  # Declare BOT_USED as global\n",
        "    print(\"Running anger management agent\")\n",
        "    anger_model = anger_management_prompt | model\n",
        "    response = anger_model.invoke(state[\"messages\"])\n",
        "    BOT_USED = \"anger_agent\"  # Assign to global variable\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def run_motivational_agent(state: State):\n",
        "    global BOT_USED  # Declare BOT_USED as global\n",
        "    print(\"Running motivational agent\")\n",
        "    motivation_model = motivational_prompt | model\n",
        "    response = motivation_model.invoke(state[\"messages\"])\n",
        "    BOT_USED = \"motivational_agent\"  # Assign to global variable\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def run_mindfulness_agent(state: State):\n",
        "    global BOT_USED  # Declare BOT_USED as global\n",
        "    print(\"Running mindfulness agent\")\n",
        "    mindfulness_model = mindfulness_prompt | model\n",
        "    response = mindfulness_model.invoke(state[\"messages\"])\n",
        "    BOT_USED = \"mindfulness_agent\"  # Assign to global variable\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Create the graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Add nodes for each agent\n",
        "workflow.add_node(\"conversational\", run_conversational_agent)\n",
        "workflow.add_node(\"suicide_prevention\", run_suicide_prevention_agent)\n",
        "workflow.add_node(\"anger_management\", run_anger_management_agent)\n",
        "workflow.add_node(\"motivational\", run_motivational_agent)\n",
        "workflow.add_node(\"mindfulness\", run_mindfulness_agent)\n",
        "\n",
        "# Add edges\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_query,\n",
        "    {\n",
        "        \"conversational\": \"conversational\",\n",
        "        \"suicide_prevention\": \"suicide_prevention\",\n",
        "        \"anger_management\": \"anger_management\",\n",
        "        \"motivational\": \"motivational\",\n",
        "        \"mindfulness\": \"mindfulness\"\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"conversational\", END)\n",
        "workflow.add_edge(\"suicide_prevention\", END)\n",
        "workflow.add_edge(\"anger_management\", END)\n",
        "workflow.add_edge(\"motivational\", END)\n",
        "workflow.add_edge(\"mindfulness\", END)\n",
        "\n",
        "# Compile the graph\n",
        "memory = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# Function to run a conversation turn\n",
        "def chat(message: str, config: dict):\n",
        "    result = graph.invoke({\"messages\": [HumanMessage(content=message)]}, config=config)\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "    # Open a CSV file for writing\n",
        "    with open('chat_responses.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"User Input\", \"Triggered Agent\", \"Response\"])  # Write the header\n",
        "\n",
        "        # i = 0\n",
        "        while True:\n",
        "            user_input = input(\"You: \")\n",
        "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "                print(\"Bot: Goodbye!\")\n",
        "                break\n",
        "\n",
        "            response = chat(user_input, config)\n",
        "            # response = response[\"messages\"].content\n",
        "            response = response['messages'][-1].content\n",
        "            print(f\"Sukoon: {response}\")\n",
        "\n",
        "            # Log the interaction to the CSV\n",
        "            writer.writerow([user_input, BOT_USED, response])  # Write the data\n",
        "            print(BOT_USED,\"**********\")\n",
        "            BOT_USED = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy0WpKeJ_mzw"
      },
      "source": [
        "# Version 2 : DBT CBT added to above 5\n",
        "## summarisation and memory added\n",
        "## Mindfulness removed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7MrCEZF2_mzx",
        "outputId": "ce1982ae-8e67-4dc0-822e-7eaab68fc5be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi, my name is manan\n",
            "\n",
            "user query :  hi, my name is manan\n",
            "Running conversational agent\n",
            "Debug Information:\n",
            "Number of Messages in Context: 2\n",
            "memory :  <langgraph.checkpoint.memory.MemorySaver object at 0x7d39f49f7bb0>\n",
            "\n",
            "Sukoon: Hi Manan, I hear you. It's nice to meet you 😊. If there's anything on your mind or something you're going through, I'm here to listen and support you.\n",
            "You: my fathers name is mahesh\n",
            "\n",
            "user query :  my fathers name is mahesh\n",
            "Running conversational agent\n",
            "Context summary needed. Initiating summarization.\n",
            "Summarizing the context...\n",
            "Memory Content for Summarization: hi, my name is manan\n",
            "Hi Manan, I hear you. It's nice to meet you 😊. If there's anything on your mind or something you're going through, I'm here to listen and support you.\n",
            "my fathers name is mahesh\n",
            "I hear you, Manan. It's nice to know a bit about your family. If there's anything else you'd like to share or talk about, I'm here to listen and support you. 😊\n",
            "Summary Response: content=\"Manan introduced himself and mentioned his father's name is Mahesh. The conversation was friendly, with an offer of support and a willingness to listen from the assistant.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 120, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_45cf54deae', 'finish_reason': 'stop', 'logprobs': None} id='run-f1737299-2224-4e74-a5fc-20edc92ff079-0' usage_metadata={'input_tokens': 120, 'output_tokens': 32, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Generated Summary: content=\"Manan introduced himself and mentioned his father's name is Mahesh. The conversation was friendly, with an offer of support and a willingness to listen from the assistant.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 120, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_45cf54deae', 'finish_reason': 'stop', 'logprobs': None} id='run-f1737299-2224-4e74-a5fc-20edc92ff079-0' usage_metadata={'input_tokens': 120, 'output_tokens': 32, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Summarization complete. Summary added to state.\n",
            "Debug Information:\n",
            "Number of Messages in Context: 5\n",
            "memory :  <langgraph.checkpoint.memory.MemorySaver object at 0x7d39f49f7bb0>\n",
            "\n",
            "Sukoon: I hear you, Manan. It's nice to know a bit about your family. If there's anything else you'd like to share or talk about, I'm here to listen and support you. 😊\n",
            "You: i am very upset, i dont want to do anything\n",
            "\n",
            "user query :  i am very upset, i dont want to do anything\n",
            "Running motivational agent\n",
            "Context summary needed. Initiating summarization.\n",
            "Summarizing the context...\n",
            "Memory Content for Summarization: hi, my name is manan\n",
            "Hi Manan, I hear you. It's nice to meet you 😊. If there's anything on your mind or something you're going through, I'm here to listen and support you.\n",
            "my fathers name is mahesh\n",
            "I hear you, Manan. It's nice to know a bit about your family. If there's anything else you'd like to share or talk about, I'm here to listen and support you. 😊\n",
            "i am very upset, i dont want to do anything\n",
            "It's completely normal to feel unmotivated sometimes. What’s the biggest challenge you're facing right now? Remember, you are important and capable of achieving anything. What’s something you’ve accomplished in the past that you're proud of? You’ve got this! What’s one small win you're aiming for today?\n",
            "Summary Response: content=\"Manan is feeling upset and unmotivated. They introduced themselves and shared that their father's name is Mahesh. The conversation offers support, encouraging Manan to share their challenges and reminding them of their past achievements and potential. It suggests focusing on small wins to help regain motivation.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 193, 'total_tokens': 249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None} id='run-fe4bacc5-e3a7-46ff-905d-37754786da61-0' usage_metadata={'input_tokens': 193, 'output_tokens': 56, 'total_tokens': 249, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Generated Summary: content=\"Manan is feeling upset and unmotivated. They introduced themselves and shared that their father's name is Mahesh. The conversation offers support, encouraging Manan to share their challenges and reminding them of their past achievements and potential. It suggests focusing on small wins to help regain motivation.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 193, 'total_tokens': 249, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None} id='run-fe4bacc5-e3a7-46ff-905d-37754786da61-0' usage_metadata={'input_tokens': 193, 'output_tokens': 56, 'total_tokens': 249, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Summarization complete. Summary added to state.\n",
            "Debug Information:\n",
            "Number of Messages in Context: 7\n",
            "memory :  <langgraph.checkpoint.memory.MemorySaver object at 0x7d39f49f7bb0>\n",
            "\n",
            "Sukoon: It's completely normal to feel unmotivated sometimes. What’s the biggest challenge you're facing right now? Remember, you are important and capable of achieving anything. What’s something you’ve accomplished in the past that you're proud of? You’ve got this! What’s one small win you're aiming for today?\n",
            "You: whats my fathers name?\n",
            "\n",
            "user query :  whats my fathers name?\n",
            "Running conversational agent\n",
            "Context summary needed. Initiating summarization.\n",
            "Summarizing the context...\n",
            "Memory Content for Summarization: hi, my name is manan\n",
            "Hi Manan, I hear you. It's nice to meet you 😊. If there's anything on your mind or something you're going through, I'm here to listen and support you.\n",
            "my fathers name is mahesh\n",
            "I hear you, Manan. It's nice to know a bit about your family. If there's anything else you'd like to share or talk about, I'm here to listen and support you. 😊\n",
            "i am very upset, i dont want to do anything\n",
            "It's completely normal to feel unmotivated sometimes. What’s the biggest challenge you're facing right now? Remember, you are important and capable of achieving anything. What’s something you’ve accomplished in the past that you're proud of? You’ve got this! What’s one small win you're aiming for today?\n",
            "whats my fathers name?\n",
            "Let's focus on how you're feeling right now. It's okay to feel upset and not want to do anything. Remember, it's important to take care of yourself. Maybe a small break or doing something you enjoy could be helpful. 😊\n",
            "Summary Response: content=\"Manan introduced himself and mentioned his father's name is Mahesh. He expressed feeling very upset and unmotivated. The conversation focused on offering support and encouraging Manan to reflect on past accomplishments and small goals to uplift his mood.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 244, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None} id='run-92701893-a251-4b52-b838-527e9b1267d6-0' usage_metadata={'input_tokens': 244, 'output_tokens': 46, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Generated Summary: content=\"Manan introduced himself and mentioned his father's name is Mahesh. He expressed feeling very upset and unmotivated. The conversation focused on offering support and encouraging Manan to reflect on past accomplishments and small goals to uplift his mood.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 244, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None} id='run-92701893-a251-4b52-b838-527e9b1267d6-0' usage_metadata={'input_tokens': 244, 'output_tokens': 46, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Summarization complete. Summary added to state.\n",
            "Debug Information:\n",
            "Number of Messages in Context: 9\n",
            "memory :  <langgraph.checkpoint.memory.MemorySaver object at 0x7d39f49f7bb0>\n",
            "\n",
            "Sukoon: Let's focus on how you're feeling right now. It's okay to feel upset and not want to do anything. Remember, it's important to take care of yourself. Maybe a small break or doing something you enjoy could be helpful. 😊\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fa898ff02a0f>\u001b[0m in \u001b[0;36m<cell line: 314>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user query : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph.message import AnyMessage, add_messages\n",
        "from typing import Literal, Annotated\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from typing import TypedDict, List\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "import yaml\n",
        "# from semantic_router import Route\n",
        "# from transformers import pipeline\n",
        "from typing import Dict\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "# Define the state\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Load prompts from YAML\n",
        "def load_prompts(file_path='/content/drive/MyDrive/prompts.yaml'):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return yaml.safe_load(file)\n",
        "\n",
        "prompts = load_prompts()\n",
        "\n",
        "# Initialize OpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "# Define prompts for different agents\n",
        "# planner_prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", \"You are a planner agent that decides which specialized agent to call based on the user's input. Respond with one of 'suicide_prevention', 'conversational', 'anger_management', 'motivational', or 'mindfulness' based on the user's emotion.\"),\n",
        "#     (\"human\", \"{input}\"),\n",
        "# ])\n",
        "\n",
        "planner_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['planner_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "conversational_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['empathetic_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "suicide_prevention_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['suicide_prevention_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "anger_management_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['anger_prevention_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "motivational_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['motivational_agent_prompt']),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# mindfulness_prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", prompts['mindfulness_agent_prompt']),\n",
        "#     (\"human\", \"{input}\"),\n",
        "# ])\n",
        "\n",
        "dialectical_behavior_therapy_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['dbt_agent_prompt']),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "cognitive_behavioral_therapy_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompts['cbt_agent_prompt']),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Define router\n",
        "def route_query(state: State):\n",
        "\n",
        "    class RouteQuery(BaseModel):\n",
        "        \"\"\"Route a user query to the most relevant node based on the emotional or psychological state identified from the query intent.\"\"\"\n",
        "\n",
        "        route: Literal[\n",
        "            \"conversational\", \"suicide_prevention\", \"anger_management\",\n",
        "            \"motivational\", \"dialectical_behavior_therapy\", \"cognitive_behavioral_therapy\"\n",
        "        ] = Field(\n",
        "            ...,\n",
        "            description=(\n",
        "                \"Choose the most appropriate agent based on the user's emotional or psychological needs, inferred from their dialogue: \"\n",
        "\n",
        "                \"'conversational' is ideal for users seeking general empathetic interaction, companionship, or simply wishing to engage in casual dialogue. This route aims to provide emotional support through open, non-directive conversation. \\n\"\n",
        "                \"Example: A user says, 'I've been feeling a bit lonely lately. I just need someone to talk to about my day.'\\n\"\n",
        "\n",
        "                \"'suicide_prevention' is critical for users who express thoughts of hopelessness, self-harm, suicidal ideation, or severe emotional distress. This route provides immediate intervention, offering resources and support to de-escalate the crisis. \\n\"\n",
        "                \"Example: A user states, 'I feel like no one would care if I were gone. I don't want to keep going anymore.'\\n\"\n",
        "\n",
        "                \"'anger_management' should be selected for users expressing frustration, irritability, or anger. This route helps the user manage their temper, process their emotions constructively, and reduce the risk of conflict escalation. \\n\"\n",
        "                \"Example: A user vents, 'I'm so mad at my boss! He keeps undermining me, and I'm about to explode.'\\n\"\n",
        "\n",
        "                \"'motivational' is suited for users who feel demotivated, struggle with low self-esteem, or are seeking encouragement to pursue their goals. This route offers positive reinforcement and practical strategies for improving self-worth and maintaining focus. \\n\"\n",
        "                \"Example: A user shares, 'I’ve been feeling stuck. Every time I try to work on my project, I lose motivation. What’s the point of even trying?' \\n\"\n",
        "\n",
        "                \"'dialectical_behavior_therapy' (DBT) should be used for users dealing with intense, fluctuating emotions or feeling emotionally overwhelmed. DBT teaches skills for emotional regulation, distress tolerance, and managing interpersonal relationships. \\n\"\n",
        "                \"Example: A user says, 'One moment I’m okay, but then I’m hit with this overwhelming sadness and anger. I don’t know how to control my emotions.'\\n\"\n",
        "\n",
        "                \"'cognitive_behavioral_therapy' (CBT) is appropriate for users struggling with negative or distorted thinking patterns, self-criticism, or irrational beliefs. CBT helps them reframe unhealthy thoughts into more positive, balanced perspectives. \\n\"\n",
        "                \"Example: A user confides, 'I always mess things up. No matter what I do, I feel like a failure, and it’s hard to think any differently.'\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    structured_llm_router = model.with_structured_output(RouteQuery)\n",
        "    question_router = planner_prompt | structured_llm_router\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    resp = question_router.invoke({\"input\": last_message})\n",
        "    return resp.route\n",
        "\n",
        "# Define agent functions\n",
        "def run_conversational_agent(state: State):\n",
        "    print(\"Running conversational agent\")\n",
        "    convo_model = conversational_prompt | model\n",
        "    response = convo_model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def run_suicide_prevention_agent(state: State):\n",
        "    print(\"Running suicide prevention agent\")\n",
        "    concern_model = suicide_prevention_prompt | model\n",
        "    response = concern_model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def run_anger_management_agent(state: State):\n",
        "    print(\"Running anger management agent\")\n",
        "    anger_model = anger_management_prompt | model\n",
        "    response = anger_model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def run_motivational_agent(state: State):\n",
        "    print(\"Running motivational agent\")\n",
        "    motivation_model = motivational_prompt | model\n",
        "    response = motivation_model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "# def run_mindfulness_agent(state: State):\n",
        "#     print(\"Running mindfulness agent\")\n",
        "#     mindfulness_model = mindfulness_prompt | model\n",
        "#     response = mindfulness_model.invoke(state[\"messages\"])\n",
        "#     return {\"messages\": response}\n",
        "\n",
        "def run_dialectical_behavior_therapy_agent(state: State):\n",
        "    print(\"Running dialectical_behavior_therapy agent\")\n",
        "    dialectical_behavior_therapy_model = dialectical_behavior_therapy_prompt | model\n",
        "    response = dialectical_behavior_therapy_model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def run_cognitive_behavioral_therapy_agent(state: State):\n",
        "    print(\"Running cognitive_behavioral_therapy agent\")\n",
        "    cognitive_behavioral_therapy_model = cognitive_behavioral_therapy_prompt | model\n",
        "    response = cognitive_behavioral_therapy_model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "# Create the graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Add nodes for each agent\n",
        "workflow.add_node(\"conversational\", run_conversational_agent)\n",
        "workflow.add_node(\"suicide_prevention\", run_suicide_prevention_agent)\n",
        "workflow.add_node(\"anger_management\", run_anger_management_agent)\n",
        "workflow.add_node(\"motivational\", run_motivational_agent)\n",
        "# workflow.add_node(\"mindfulness\", run_mindfulness_agent)\n",
        "workflow.add_node(\"dialectical_behavior_therapy\", run_dialectical_behavior_therapy_agent)\n",
        "workflow.add_node(\"cognitive_behavioral_therapy\", run_cognitive_behavioral_therapy_agent)\n",
        "\n",
        "# Add edges\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_query,\n",
        "    {\n",
        "        \"conversational\": \"conversational\",\n",
        "        \"suicide_prevention\": \"suicide_prevention\",\n",
        "        \"anger_management\": \"anger_management\",\n",
        "        \"motivational\": \"motivational\",\n",
        "        # \"mindfulness\": \"mindfulness\",\n",
        "        \"dialectical_behavior_therapy\": \"dialectical_behavior_therapy\",\n",
        "        \"cognitive_behavioral_therapy\": \"cognitive_behavioral_therapy\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"conversational\", END)\n",
        "workflow.add_edge(\"suicide_prevention\", END)\n",
        "workflow.add_edge(\"anger_management\", END)\n",
        "workflow.add_edge(\"motivational\", END)\n",
        "# workflow.add_edge(\"mindfulness\", END)\n",
        "workflow.add_edge(\"dialectical_behavior_therapy\", END)\n",
        "workflow.add_edge(\"cognitive_behavioral_therapy\", END)\n",
        "\n",
        "# Compile the graph\n",
        "memory = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "\n",
        "\n",
        "# # Function to run a conversation turn\n",
        "# def chat(message: str, config: dict):\n",
        "#     result = graph.invoke({\"messages\": [HumanMessage(content=message)]}, config=config)\n",
        "#     return result[\"messages\"][-1]\n",
        "\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define the summarization prompt\n",
        "summarization_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Summarize the following memory of the conversation into a concise and relevant context summary to retain key points.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Fixing the memory content extraction\n",
        "def generate_summary(state):\n",
        "    summarizer = summarization_prompt | model\n",
        "    # Accessing content directly from each message in state[\"messages\"]\n",
        "    memory_content = \"\\n\".join([msg.content if hasattr(msg, 'content') else str(msg) for msg in state[\"messages\"]])\n",
        "    print(\"Memory Content for Summarization:\", memory_content)  # Debug log\n",
        "    summary_response = summarizer.invoke({\"input\": memory_content})\n",
        "    print(\"Summary Response:\", summary_response)  # Debug log\n",
        "    return {\"summary\": summary_response}\n",
        "\n",
        "\n",
        "\n",
        "# Adding the summarized context to the state\n",
        "def add_summarized_context(state: State):\n",
        "    print(\"Summarizing the context...\")\n",
        "\n",
        "    # Generate the summary of the current conversation memory\n",
        "    summary = generate_summary(state)\n",
        "\n",
        "    # Create a system message with the summary and update the state\n",
        "    # Extract the summary as a plain string from the response dictionary\n",
        "    if isinstance(summary, dict) and \"summary\" in summary:\n",
        "     summary_content = summary[\"summary\"].get(\"content\", \"\") if isinstance(summary[\"summary\"], dict) else summary[\"summary\"]\n",
        "    else:\n",
        "     summary_content = str(summary)\n",
        "    if isinstance(summary[\"summary\"], AIMessage):\n",
        "     summary_content = summary[\"summary\"].content\n",
        "    elif isinstance(summary[\"summary\"], dict) and \"content\" in summary[\"summary\"]:\n",
        "     summary_content = summary[\"summary\"][\"content\"]\n",
        "    else:\n",
        "     summary_content = str(summary[\"summary\"])  # Convert to string as a fallback\n",
        "    if isinstance(summary_content, list):\n",
        "     summary_content = \" \".join([str(item) for item in summary_content if isinstance(item, str)])\n",
        "\n",
        "\n",
        "    summary_message = SystemMessage(content=str(summary_content))\n",
        "    state[\"messages\"].append(summary_message)\n",
        "\n",
        "    # Log the summary for debugging\n",
        "    print(f\"Generated Summary: {summary['summary']}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "# # Ensure all messages passed to the chat function are strings\n",
        "# def format_messages(messages):\n",
        "#     formatted_messages = []\n",
        "#     for msg in messages:\n",
        "#         if isinstance(msg, (AIMessage, HumanMessage, SystemMessage)):\n",
        "#             formatted_messages.append(msg.content)\n",
        "#         elif isinstance(msg, tuple):\n",
        "#             # Safely convert tuple to a string\n",
        "#             formatted_messages.append(str(msg))\n",
        "#         else:\n",
        "#             formatted_messages.append(str(msg))\n",
        "#     return formatted_messages\n",
        "\n",
        "# Update the chat function to use summarization\n",
        "# Refactored chat function with integrated summarization\n",
        "# Get the last message as the response, ignoring summarization system messages\n",
        "def get_final_response(messages):\n",
        "    # Find the last non-summary message (ignoring system messages added during summarization)\n",
        "    for msg in reversed(messages):\n",
        "        if not isinstance(msg, SystemMessage):  # Exclude SystemMessage (summary)\n",
        "            return msg\n",
        "    return messages[-1]  # Fallback to the last message\n",
        "\n",
        "# Refactored chat function with integrated summarization\n",
        "def chat(message: str, config: dict):\n",
        "    # Initialize state with user message\n",
        "    state = {\"messages\": [HumanMessage(content=message)]}\n",
        "\n",
        "    # Invoke the graph to process user input and obtain response\n",
        "    result = graph.invoke(state, config=config)\n",
        "\n",
        "    # Check if summarization is needed\n",
        "    if len(result[\"messages\"]) > 3:  # Trigger summarization if message count exceeds threshold\n",
        "        print(\"Context summary needed. Initiating summarization.\")\n",
        "        summarized_state = add_summarized_context(result)\n",
        "        print(\"Summarization complete. Summary added to state.\")\n",
        "    else:\n",
        "        summarized_state = result  # Use the result without summarization\n",
        "\n",
        "    # Extract the final user-facing response\n",
        "    response_message = get_final_response(summarized_state[\"messages\"])\n",
        "\n",
        "    # Print internal state for debugging\n",
        "    print(\"Debug Information:\")\n",
        "    print(f\"Number of Messages in Context: {len(summarized_state['messages'])}\")\n",
        "\n",
        "    # Return the last generated response\n",
        "    return response_message\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "    i = 0\n",
        "    while True and i<5:\n",
        "        i+=1\n",
        "        user_input = input(\"You: \")\n",
        "        print(\"\")\n",
        "        print(\"user query : \",user_input)\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Bot: Goodbye!\")\n",
        "            break\n",
        "        response = chat(user_input, config)\n",
        "        print(\"memory : \",memory)\n",
        "        print(\"\")\n",
        "        print(\"Sukoon:\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MshVP6X_mz2",
        "outputId": "8f0a79b0-116a-4c5b-f1cc-4ae4354f69b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Goodbye!\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqRZpw6q_mz2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}